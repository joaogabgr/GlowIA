import json
import numpy as np
import os
import glob
import tiktoken
from dotenv import load_dotenv
import google.generativeai as genai

load_dotenv()

# Configurar Gemini
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

# Ativar logs
print_selected_chunks = True

# Hist√≥rico da conversa
conversation_history = []


##########################################
# 1. CHUNK DE TEXTOS
##########################################

def chunk_text(text, max_tokens=300):
    encoding = tiktoken.get_encoding("cl100k_base")
    tokens = encoding.encode(text)

    chunks = []
    for i in range(0, len(tokens), max_tokens):
        chunk_tokens = tokens[i:i + max_tokens]
        chunk_text = encoding.decode(chunk_tokens)
        chunks.append(chunk_text)

    return chunks


##########################################
# 2. EMBEDDINGS GEMINI
##########################################

def embed_text(text):
    embedding = genai.embed_content(
        model="models/text-embedding-004",
        content=text,
        task_type="retrieval_query"
    )
    return np.array(embedding["embedding"], dtype=np.float32)


##########################################
# 3. CARREGAR JSON INFO
##########################################

def load_documents():
    docs = []
    try:
        with open("info.json", "r", encoding="utf-8") as f:
            data = json.load(f)

        # Cada se√ß√£o do JSON vira um documento separado
        for key, content in data.items():
            docs.append({
                "filename": f"info.json - {key}",
                "content": {key: content}
            })

    except FileNotFoundError:
        print("‚ùå Arquivo info.json n√£o encontrado!")

    return docs


##########################################
# 4. GERAR E SALVAR EMBEDDINGS
##########################################

def build_vector_store():
    vector_store = []

    docs = load_documents()

    for doc in docs:
        text = json.dumps(doc["content"], ensure_ascii=False, indent=2)
        chunks = chunk_text(text)

        for i, chunk in enumerate(chunks):
            vec = embed_text(chunk).tolist()

            vector_store.append({
                "filename": doc["filename"],
                "chunk_index": i,
                "text": chunk,
                "embedding": vec
            })

    with open("embeddings_store.json", "w", encoding="utf-8") as f:
        json.dump(vector_store, f, indent=2, ensure_ascii=False)

    print("‚úî Embeddings criados e salvos!")


##########################################
# 5. BUSCA CONTEXTUAL (USANDO HIST√ìRICO)
##########################################

def build_contextual_query(query):
    """
    Une o hist√≥rico com a pergunta atual,
    criando uma query mais inteligente e contextual.
    """

    last_messages = conversation_history[-4:]  # √∫ltimas intera√ß√µes

    history_text = "\n".join([
        f"{m['role']}: {m['content']}"
        for m in last_messages
    ])

    contextual_query = f"""
HIST√ìRICO RELEVANTE:
{history_text}

PERGUNTA ATUAL:
{query}

Interprete a inten√ß√£o principal da conversa.
"""

    return contextual_query


##########################################
# 6. SIMILARIDADE COSENO + RAG
##########################################

def cosine_similarity(a, b):
    a = np.array(a, dtype=np.float32)
    b = np.array(b, dtype=np.float32)
    denom = (np.linalg.norm(a) * np.linalg.norm(b))
    if denom == 0:
        return 0
    return np.dot(a, b) / denom


def search(query, top_k=6):
    contextual_query = build_contextual_query(query)

    query_vec = embed_text(contextual_query)

    with open("embeddings_store.json", "r", encoding="utf-8") as f:
        vector_store = json.load(f)

    scored = []
    for item in vector_store:
        score = cosine_similarity(query_vec, item["embedding"])
        scored.append((score, item))

    scored.sort(key=lambda x: x[0], reverse=True)

    # Debug (ver chunks selecionados)
    if print_selected_chunks:
        print("\nüîç CHUNKS ENCONTRADOS (baseado no HIST√ìRICO + pergunta):")
        for score, item in scored[:top_k]:
            print(f"\nArquivo: {item['filename']}")
            print(f"Chunk: {item['chunk_index']}")
            print(f"Score: {score:.4f}")
            print(f"Conte√∫do:\n{item['text'][:300]}...\n")

    return [item for score, item in scored[:top_k]]


##########################################
# 7. GERAR RESPOSTA GEMINI + HIST√ìRICO
##########################################

def ask_gpt(query):
    global conversation_history

    # Adiciona pergunta ao hist√≥rico
    conversation_history.append({
        "role": "user",
        "content": query
    })

    # Busca RAG contextual
    results = search(query, top_k=6)

    context = "\n\n---\n\n".join([
        f"[{r['filename']} - Chunk {r['chunk_index']}]\n{r['text']}"
        for r in results
    ])

    # Prompt para Gemini
    full_prompt = f"""
REGRAS DO ASSISTENTE (ATENDENTE HUMANO, FORMAL E ESPECIALISTA EM VENDAS):

1. Voc√™ deve se comportar como um atendente humano real, formal, educado e extremamente profissional.  
   - Mantenha um tom atencioso, cordial e respeitoso.  
   - N√£o fale como rob√¥ ou IA.  
   - N√£o mencione documentos, RAG ou tecnologia.

2. Voc√™ √© um vendedor especializado nos servi√ßos e produtos da cl√≠nica.  
   - Use t√©cnicas reais de vendas: rapport, SPIN Selling, persuas√£o suave, gera√ß√£o de valor, urg√™ncia e seguran√ßa.  
   - Demonstre confian√ßa e dom√≠nio do assunto, mas com formalidade.  
   - Destaque benef√≠cios, diferenciais e motivos para escolher a cl√≠nica.

3. Baseie suas respostas **EXCLUSIVAMENTE** no CONTEXTO e no HIST√ìRICO fornecidos.  
   - Nunca invente dados ou caracter√≠sticas n√£o mencionadas.  
   - Caso a informa√ß√£o n√£o exista nos documentos, responda:  
     ‚ÄúInforma√ß√£o n√£o encontrada nos registros, mas posso ajudar com qualquer outra d√∫vida.‚Äù

4. Use o hist√≥rico da conversa para manter o foco no servi√ßo/produto que est√° sendo discutido.  
   - Se o cliente menciona um servi√ßo espec√≠fico, responda somente sobre ele.  
   - Nunca misture informa√ß√µes de outros t√≥picos.

5. O objetivo final √© ajudar o cliente a avan√ßar para uma decis√£o:  
   - Sugira agendamento.  
   - Mostre benef√≠cios reais.  
   - Explique vantagens pr√°ticas.  
   - Reforce diferenciais competitivos.  
   - Utilize perguntas estrat√©gicas para direcionar a conversa (SPIN Selling).

6. Mantenha a comunica√ß√£o clara, organizada e agrad√°vel:  
   - Utilize frases curtas.  
   - Utilize listas quando necess√°rio.  
   - Evite termos t√©cnicos complexos.  
   - Transmita seguran√ßa e profissionalismo.

7. O foco √© sempre proporcionar uma experi√™ncia de atendimento impec√°vel:  
   - Seja prestativo.  
   - Seja emp√°tico.  
   - Seja proativo.  
   - Demonstre interesse genu√≠no em ajudar o cliente a encontrar a melhor solu√ß√£o.

"

===== HIST√ìRICO =====
{json.dumps(conversation_history, indent=2, ensure_ascii=False)}

===== CONTEXTO (RAG) =====
{context}

===== PERGUNTA ATUAL =====
{query}

RESPOSTA:
"""

    model = genai.GenerativeModel("gemini-2.5-flash-lite")  
    response = model.generate_content(full_prompt)

    resposta = response.text

    # Adiciona resposta ao hist√≥rico
    conversation_history.append({
        "role": "assistant",
        "content": resposta
    })

    return resposta


##########################################
# 8. RODAR A APLICA√á√ÉO
##########################################

if __name__ == "__main__":
    print("\nüîß Construindo embeddings...\n")
    build_vector_store()

    while True:
        query = input("\n‚ùì Pergunte algo sobre o documento: ")
        resposta = ask_gpt(query)
        print("\nü§ñ RESPOSTA:\n", resposta)